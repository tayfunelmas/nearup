import hashlib
import json
import logging
import os
import site
import shutil
import subprocess
import sys
import tempfile

import psutil

from nearuplib.constants import DEFAULT_WAIT_TIMEOUT, LOGS_FOLDER, NODE_PID_FILE
from nearuplib.util import (download_binaries, download_genesis,
                            latest_genesis_md5sum, read_genesis_md5sum,
                            write_genesis_md5sum, new_release_ready,
                            prompt_bool_flag, prompt_flag, wraptext)
from nearuplib.watcher import is_watcher_running, run_watcher, stop_watcher
from nearuplib.tailer import next_logname


def read_validator_key(home_dir):
    key_path = os.path.join(home_dir, 'validator_key.json')
    try:
        with open(key_path) as key_file:
            return json.load(key_file)
    except FileNotFoundError:
        return None


def print_validator_info(home_dir):
    key_data = read_validator_key(home_dir)
    if key_data is None:
        logging.warning('Validator key not generated by neard as expected')
        return

    print()
    print(
        wraptext(f'''
        The validator’s public key is {key_data["public_key"]}

        The node will now run and will sync with the network, but this will not
        automatically initiate staking.  In order to do that, you need to send
        a transaction to the network indicating your intent to stake.  Please
        see
        https://docs.near.org/docs/develop/node/validator/staking-and-delegation.

        If you have near-cli (https://github.com/near/near-cli) installed, you
        can do this with the following command:
    '''))
    print()
    print(f'    near stake {key_data["account_id"]} {key_data["public_key"]} '
          '<amount>')
    print()


def init_near(home_dir, binary_path, chain_id, account_id, interactive=False):
    logging.info("Initializing the node configuration using near binary...")

    msg = wraptext('''
        Would you like to initialize the NEAR node as a validator? If so, this
        will generate an extra key pair you’ll be able to use to stake NEAR as
        a validator, which is distinct from the keys you'd use in normal
        transactions.
    ''')
    if prompt_bool_flag(msg, bool(account_id), interactive=interactive):
        # TODO: check that the account exists and warn if not
        msg = '''
            Enter an account ID. This should be an existing account you’d like
            to use to stake NEAR as a validator. If you don’t already have an
            account, please see
            https://docs.near.org/docs/develop/basics/create-account
        '''
        account_id = prompt_flag(msg,
                                 account_id,
                                 default=None,
                                 interactive=interactive,
                                 type=str)
    else:
        account_id = None

    cmd = [
        f'{binary_path}/neard', f'--home={home_dir}', 'init',
        f'--chain-id={chain_id}'
    ]
    if account_id:
        cmd.append(f'--account-id={account_id}')
    if chain_id in ['betanet', 'testnet', 'shardnet']:
        cmd.append('--download-genesis')
        cmd.append('--download-config')
        genesis_md5sum = latest_genesis_md5sum(chain_id)

    while True:
        if interactive:
            print(f'Running "{" ".join(cmd)}"')
        subprocess.check_call(cmd)

        if chain_id in ['betanet', 'shardnet', 'testnet']:
            new_genesis_md5sum = latest_genesis_md5sum(chain_id)

            if new_genesis_md5sum != genesis_md5sum:
                genesis_md5sum = new_genesis_md5sum
                logger.info(
                    f'genesis md5sum changed while neard init was running. reinitializing...'
                )
                shutil.rmtree(home_dir)
                continue
            else:
                write_genesis_md5sum(home_dir, genesis_md5sum)

        break

    if interactive:
        print_validator_info(home_dir)


def genesis_changed(chain_id, home_dir):
    genesis_md5sum, records_md5sum = latest_genesis_md5sum(chain_id)

    local_genesis_md5sum, local_records_md5sum = read_genesis_md5sum(home_dir)

    if genesis_md5sum != local_genesis_md5sum:
        logging.info(
            f'genesis md5sum has changed. ours: {local_genesis_md5sum} remote: {genesis_md5sum}'
        )
        return True

    # we assume if remote shows no records but we have a records.json for some reason, with
    # the same genesis that the remote shows, we probably dont want to redownload stuff
    if records_md5sum is None:
        return False

    ret = records_md5sum != local_records_md5sum
    if ret:
        logging.info(
            f'records md5sum has changed. ours: {local_records_md5sum} remote: {records_md5sum}'
        )
    return ret


# only meant to be called when the old genesis has records in the genesis file, and the new genesis
# has records in a separate file. returns true if all fields of the genesis are the same as well as the records
# (without trying to sort them or be clever at all)
def genesis_files_equivalent(old_genesis_path, new_genesis_path,
                             new_records_path):
    with open(old_genesis_path, 'r') as old_genesis, open(
            new_genesis_path, 'r') as new_genesis, open(new_records_path,
                                                        'r') as new_records:
        old_genesis = json.load(old_genesis)
        new_genesis = json.load(new_genesis)

        seen_keys = set()
        for key in old_genesis:
            if key == 'records':
                continue

            seen_keys.add(key)
            try:
                new_value = new_genesis[key]
            except KeyError:
                return False

            old_value = old_genesis[key]

            if old_value != new_value:
                return False

        for key in new_genesis:
            if key == 'records':
                continue

            if key not in seen_keys:
                return False

        h = hashlib.sha256()

        for record in old_genesis['records']:
            h.update(json.dumps(record).encode('utf-8'))
        old_records_hash = h.digest()

        # actually del it because it's really big for testnet. would be nice to read thru
        # these sequentially instead of all at once into a giant object
        del old_genesis
        new_records = json.load(new_records)

        h = hashlib.sha256()
        for record in new_records:
            h.update(json.dumps(record).encode('utf-8'))
        new_records_hash = h.digest()

        return new_records_hash == old_records_hash


def check_and_update_genesis(chain_id, home_dir, binary_path):
    if genesis_changed(chain_id, home_dir):
        logging.info(
            f'Update genesis config and remove stale data for {chain_id}')

        with tempfile.TemporaryDirectory() as tmp_dir:
            init_near(tmp_dir, binary_path, chain_id, None, interactive=False)

            with open(os.path.join(tmp_dir, 'config.json'), 'r') as config_fd:
                config = json.load(config_fd)
                config_has_records = config['genesis_records_file'] is not None
                records_downloaded = os.path.exists(
                    os.path.join(tmp_dir, 'records.json'))
                if config_has_records != records_downloaded:
                    if config_has_records:
                        logging.warning(
                            f'newly downloaded config shows records needed, but neard init did not download records.json'
                        )
                    else:
                        logging.warning(
                            f'newly downloaded config shows records not needed, but neard init downloaded records.json'
                        )

            with open(os.path.join(home_dir, 'config.json'), 'r+') as config_fd:
                config = json.load(config_fd)
                config_had_records = config['genesis_records_file'] is not None
                if config_has_records != config_had_records:
                    config[
                        'genesis_records_file'] = 'records.json' if config_has_records else None
                    config_fd.seek(0)
                    json.dump(config, config_fd, indent=2)
                    config_fd.truncate()

            if os.path.exists(os.path.join(home_dir, 'data')):
                # for now the only situation where we try to check if we can avoid deleting the data dir is the case
                # of moving from no records file to having a records file, since it's best effort anyway and it's the
                # only likely case for the moment
                keep_data = not config_had_records and config_has_records and records_downloaded and genesis_files_equivalent(
                    os.path.join(home_dir, 'genesis.json'),
                    os.path.join(tmp_dir, 'genesis.json'),
                    os.path.join(tmp_dir, 'records.json'))
                if not keep_data:
                    shutil.rmtree(os.path.join(home_dir, 'data'))

            shutil.move(os.path.join(tmp_dir, 'genesis.json'),
                        os.path.join(home_dir, 'genesis.json'))
            if records_downloaded:
                shutil.move(os.path.join(tmp_dir, 'records.json'),
                            os.path.join(home_dir, 'records.json'))
            # TODO: would be sad to get ^C between moving the above files and moving these two.
            # would be good to handle that somehow
            shutil.move(os.path.join(tmp_dir, '.nearup/genesis_md5sum'),
                        os.path.join(home_dir, '.nearup/genesis_md5sum'))
            try:
                shutil.move(os.path.join(tmp_dir, '.nearup/records_md5sum'),
                            os.path.join(home_dir, '.nearup/records_md5sum'))
            except FileNotFoundError:
                pass

        return True

    return False


def check_and_setup(binary_path,
                    home_dir,
                    chain_id,
                    account_id,
                    interactive=False):
    """Checks if there is already everything setup on this machine, otherwise sets up NEAR node."""
    if os.path.exists(os.path.join(home_dir)):
        if chain_id != 'localnet':
            with open(os.path.join(home_dir, 'genesis.json')) as genesis_fd:
                genesis_config = json.loads(genesis_fd.read())
                if genesis_config['chain_id'] != chain_id:
                    logging.error(
                        f"{home_dir} has wrong network configuration in genesis."
                        f"Specify different --home or specify the correct genesis.",
                    )
                    sys.exit(1)

        if chain_id in ['guildnet', 'betanet', 'testnet', 'shardnet']:
            check_and_update_genesis(chain_id, home_dir, binary_path)
        elif chain_id == 'mainnet':
            logging.info("Using the mainnet genesis...")
        else:
            logging.info("Using existing node configuration from %s for %s",
                         home_dir, chain_id)
        return

    logging.info("Setting up network configuration.")
    init_near(home_dir, binary_path, chain_id, account_id, interactive)

    if chain_id not in [
            'mainnet', 'guildnet', 'betanet', 'testnet', 'shardnet'
    ]:
        with open(os.path.join(home_dir, 'genesis.json'), 'r+') as genesis_fd:
            genesis_config = json.load(genesis_fd)
            genesis_config['gas_price'] = 0
            genesis_config['min_gas_price'] = 0
            json.dump(genesis_config, genesis_fd)


def print_staking_key(home_dir):
    key_data = read_validator_key(home_dir)
    if key_data is None:
        return

    if not key_data['account_id']:
        logging.warning(
            "Node is not staking. Re-run init to specify staking account.")
        return
    logging.info("Stake for user '%s' with '%s'", key_data['account_id'],
                 key_data['public_key'])


def run_binary(path,
               home,
               action,
               neard_log=None,
               verbose=False,
               shards=None,
               validators=None,
               non_validators=None,
               boot_nodes=None,
               output=None,
               print_command=False,
               fixed_shards=False,
               archival_nodes=None,
               rpc_nodes=None,
               tracked_shards=False,
               opentelemetry=None):
    command = [path, '--home', str(home)]

    env = os.environ.copy()
    env['RUST_BACKTRACE'] = '1'

    # Note, we need to make these options mutually exclusive
    # for backwards capability reasons, until v1.0.0
    if verbose:
        env['RUST_LOG'] = 'debug,actix_web=info'
    elif neard_log:
        env['RUST_LOG'] = neard_log

    if opentelemetry:
        env['OTEL_EXPORTER_OTLP_TRACES_ENDPOINT'] = "http://127.0.0.1:4317"

    command.append(action)

    if shards:
        command.extend(['--shards', str(shards)])
    if validators:
        command.extend(['--v', str(validators)])
    if non_validators:
        command.extend(['--n', str(non_validators)])
    if boot_nodes:
        command.extend(['--boot-nodes', boot_nodes])
    if fixed_shards:
        command.append('--fixed-shards')
    if archival_nodes:
        command.extend(['--archival-nodes', archival_nodes])
    if rpc_nodes:
        command.extend(['--rpc-nodes', rpc_nodes])
    if tracked_shards:
        command.extend(['--tracked-shards', tracked_shards])

    if output:
        logname = f'{output}.log'
        if logname != next_logname(logname):
            shutil.move(logname, next_logname(logname))
        output = open(logname, 'w')

    if print_command:
        print(f'Running "{" ".join(command)}"')
    neard = subprocess.Popen(command, stderr=output, stdout=output, env=env)
    return neard


def proc_name_from_pid(pid):
    process = psutil.Process(pid)
    return process.name()


def is_neard_running():
    if os.path.exists(NODE_PID_FILE):
        logging.error("There is already binary nodes running.")
        logging.error("Either run nearup stop or by kill the process manually.")
        logging.warning(f"If this is a mistake, remove {NODE_PID_FILE}")
        return True
    return False


def run(home_dir,
        binary_path,
        boot_nodes,
        neard_log,
        verbose,
        chain_id,
        print_command=False,
        watch=False):
    proc = run_binary(os.path.join(binary_path, 'neard'),
                      home_dir,
                      'run',
                      neard_log=neard_log,
                      verbose=verbose,
                      boot_nodes=boot_nodes,
                      output=os.path.join(LOGS_FOLDER, chain_id),
                      print_command=print_command)
    proc_name = proc_name_from_pid(proc.pid)

    with open(NODE_PID_FILE, 'w') as pid_fd:
        pid_fd.write(f"{proc.pid}|{proc_name}|{chain_id}")
        pid_fd.close()

    logging.info("Node is running...")
    logging.info("To check logs call: `nearup logs` or `nearup logs --follow`")

    if watch:
        logging.info("Watcher is enabled. Starting watcher...")
        run_watcher(chain_id, home=home_dir)


def setup_and_run(binary_path,
                  home_dir,
                  boot_nodes,
                  chain_id,
                  account_id=None,
                  verbose=False,
                  interactive=False,
                  neard_log='',
                  watcher=True):
    logging.info(
        f'setup and run, chain_id: {chain_id} binary_path: {binary_path}')

    if is_neard_running():
        sys.exit(1)

    if watcher and is_watcher_running():
        sys.exit(1)

    if binary_path == '':
        logging.info('Using officially compiled binary')
        uname = os.uname()[0]
        if uname not in ['Darwin', 'Linux']:
            logging.error(
                'Sorry your Operating System does not have officially compiled binary now.'
            )
            logging.error(
                'Compile a local binary and set --binary-path when running')
            sys.exit(1)

        binary_path = os.path.expanduser(f'~/.nearup/near/{chain_id}')
        if not os.path.exists(binary_path):
            os.makedirs(binary_path)

        download_binaries(chain_id, uname)
    else:
        logging.info(f'Using local binary at {binary_path}')
        watcher = False  # ensure watcher doesn't run and try to download official binaries

    check_and_setup(binary_path, home_dir, chain_id, account_id, interactive)

    print_staking_key(home_dir)
    run(home_dir,
        binary_path,
        boot_nodes,
        neard_log,
        verbose,
        chain_id,
        watch=watcher,
        print_command=interactive)


def stop_nearup(keep_watcher=False):
    logging.warning("Stopping the near daemon...")
    stop_native()

    if not keep_watcher:
        logging.warning("Stopping the nearup watcher...")
        stop_watcher()
    else:
        logging.warning("Skipping the stopping of the nearup watcher...")


def restart_nearup(net,
                   path=os.path.join(site.USER_BASE, 'bin/nearup'),
                   home_dir='',
                   keep_watcher=True,
                   verbose=False,
                   restart_only_new_version=True):
    logging.warning("Restarting nearup...")

    if not os.path.exists(path):
        logging.error(
            "Please delete current nearup and install the new with `pip3 install --user nearup`"
        )
        logging.error(
            "For local development use: `pip3 install --user .` from root directory"
        )
        sys.exit(1)

    uname = os.uname()[0]
    if restart_only_new_version and not new_release_ready(net, uname):
        logging.warning(
            f'Latest release for {net} is not ready. Skipping restart.')
        return

    logging.warning("Stopping nearup...")
    stop_nearup(keep_watcher=keep_watcher)

    logging.warning("Starting nearup...")
    setup_and_run(binary_path='',
                  home_dir=home_dir,
                  chain_id=net,
                  boot_nodes='',
                  verbose=verbose,
                  watcher=not keep_watcher)

    logging.info("Nearup has been restarted...")


def stop_native(timeout=DEFAULT_WAIT_TIMEOUT):
    try:
        if os.path.exists(NODE_PID_FILE):
            with open(NODE_PID_FILE) as pid_file:
                for line in pid_file.readlines():
                    pid, proc_name, _ = line.strip().split("|")
                    pid = int(pid)
                    process = psutil.Process(pid)
                    logging.info(
                        f"Near procces is {proc_name} with pid: {pid}...")

                    if proc_name in proc_name_from_pid(pid):
                        logging.info(
                            f"Stopping process {proc_name} with pid {pid}...")
                        try:
                            process.terminate()
                            process.wait(timeout=timeout)
                        except psutil.TimeoutExpired:
                            logging.warning(
                                f"Timeout expired. Killing process {pid}")
                            process.kill()

            os.remove(NODE_PID_FILE)
        else:
            logging.info("Near deamon is not running...")
    except Exception as ex:
        logging.error(f"There was an error while stopping watcher: {ex}")
        if os.path.exists(NODE_PID_FILE):
            os.remove(NODE_PID_FILE)


def is_neard_zombie():
    if not is_neard_running():
        return False
    with open(NODE_PID_FILE) as pid_file:
        for line in pid_file.readlines():
            pid, proc_name, _ = line.strip().split("|")
            pid = int(pid)
            try:
                process = psutil.Process(pid)
                logging.info(f"Near procces is {proc_name} with pid: {pid}...")
                if proc_name not in process.name():
                    return True
                if process.status() in [
                        psutil.STATUS_ZOMBIE, psutil.STATUS_DEAD
                ]:
                    return True
            except psutil.NoSuchProcess:
                return True
    return False
